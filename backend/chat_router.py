import asyncio # âœ… æ–°å¢åŒ¯å…¥ï¼Œç”¨æ–¼å¾Œå°ä»»å‹™
from fastapi import APIRouter, HTTPException, BackgroundTasks # âœ… åŒ¯å…¥ BackgroundTasks
from pydantic import BaseModel
from typing import Optional
import os
import logging
import json
import traceback # ç¢ºä¿ traceback ä¹Ÿåœ¨

# *** è«‹ç¢ºä¿é€™äº›æ¨¡çµ„åœ¨ä½ çš„ backend/ ç›®éŒ„ä¸­å¯è¢«æ­£ç¢ºåŒ¯å…¥ ***
from backend.supabase_handler import get_supabase
supabase = get_supabase()
from backend.openai_handler import get_openai_client, generate_response
from backend.prompt_engine import PromptEngine
from modules.memory_system import MemorySystem
from backend.modules.memory.redis_interface import RedisInterface
from backend.core_controller import get_core_controller # ç¢ºä¿ core_controller æ”¾åœ¨é ‚å±¤

router = APIRouter()
logger = logging.getLogger("chat_router")
redis_interface = RedisInterface()

_new_memory_core = None
_reflection_storage = None

def get_new_memory_core():
    """ç²å–æ–°è¨˜æ†¶æ¨¡çµ„æ ¸å¿ƒï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰"""
    global _new_memory_core
    if _new_memory_core is None:
        try:
            from backend.modules.memory.core import MemoryCore
            _new_memory_core = MemoryCore()
            logger.info("âœ… æ–°è¨˜æ†¶æ¨¡çµ„å·²å•Ÿç”¨")
        except Exception as e:
            logger.warning(f"âš ï¸ æ–°è¨˜æ†¶æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
            _new_memory_core = None
    return _new_memory_core

def get_reflection_storage():
    """ç²å–åæ€å„²å­˜æœå‹™ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰"""
    global _reflection_storage
    if _reflection_storage is None:
        try:
            from backend.modules.reflection_storage import ReflectionStorage
            from backend.modules.memory.redis_interface import RedisInterface
            from backend.modules.pinecone_handler import PineconeHandler
            
            redis_interface = RedisInterface()
            pinecone_handler = PineconeHandler()
            
            _reflection_storage = ReflectionStorage(
                redis_interface=redis_interface,
                supabase_client=supabase,
                pinecone_handler=pinecone_handler
            )
            logger.info("âœ… åæ€å„²å­˜æœå‹™å·²å•Ÿç”¨")
        except Exception as e:
            logger.warning(f"âš ï¸ åæ€å„²å­˜æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
            _reflection_storage = None
    return _reflection_storage


# =========================================================
# âœ… æ•¸æ“šæ¨¡å‹ï¼ˆå·²æ–°å¢ AI å¯¶è²åˆ‡æ›é–‹é—œï¼‰
# =========================================================
class ChatRequest(BaseModel):
    user_message: str
    conversation_id: str
    user_id: str = "default_user"
    # âœ… æ–°å¢ AI å¯¶è²åˆ‡æ›é–‹é—œ
    ai_id: str = os.getenv("AI_ID", "xiaochenguang_v1") 
    
class ChatResponse(BaseModel):
    assistant_message: str
    emotion_analysis: dict
    conversation_id: str
    reflection: Optional[dict] = None 


# =========================================================
# âœ… æ ¸å¿ƒï¼šã€éš±å½¢å¾Œé–€é€šé“ã€‘çš„è™•ç†å‡½æ•¸ (Background Task Function)
# =========================================================
async def run_post_chat_tasks(
    request: ChatRequest, assistant_message: str, emotion_analysis: dict
):
    """
    æ­¤å‡½æ•¸è² è²¬æ‰€æœ‰è€—æ™‚çš„ã€ä¸å½±éŸ¿å³æ™‚å›è¦†çš„å¾ŒçºŒè™•ç†å·¥ä½œï¼š
    åæ€ã€è¡Œç‚ºèª¿ç¯€ã€ä¸‰å±¤è¨˜æ†¶å„²å­˜ç­‰ã€‚
    """
    logger.info(f"ğŸŸ¢ å•Ÿå‹•èƒŒæ™¯è™•ç†ä»»å‹™ï¼Œè™•ç† conversation_id: {request.conversation_id}")
    
    # é‡æ–°å¯¦ä¾‹åŒ–æˆ–ç²å–å¿…è¦çš„æœå‹™
    openai_client = get_openai_client()
    memories_table = os.getenv("SUPABASE_MEMORIES_TABLE", "xiaochenguang_memories")
    memory_system = MemorySystem(supabase, openai_client, memories_table)
    prompt_engine = PromptEngine(request.conversation_id, memories_table)
    
    reflection_result = None
    
    try:
        controller = await get_core_controller()
        
        # é¡å¤–ï¼šå°‡å³æ™‚å„²å­˜ä¹Ÿæ”¾å…¥èƒŒæ™¯ï¼Œåªåœ¨å›è¦†å¾ŒåŸ·è¡Œ
        await memory_system.save_emotional_state(
            request.user_id,
            emotion_analysis,
            context=request.user_message
        )
        prompt_engine.personality_engine.learn_from_interaction(
            request.user_message,
            assistant_message,
            emotion_analysis
        )
        await asyncio.to_thread(prompt_engine.personality_engine.save_personality) # ç¢ºä¿åŒæ­¥å¯«å…¥è¢«å®‰å…¨è™•ç†
        
        # === éšæ®µ1ï¼šåæ€åˆ†æ ===
        reflection_module = await controller.get_module("reflection")
        if reflection_module:
            reflection_response = await reflection_module.process({
                "user_message": request.user_message,
                "assistant_message": assistant_message,
                "emotion_analysis": emotion_analysis
            })
            
            if reflection_response.get("success"):
                reflection_result = reflection_response.get("reflection")
                logger.info(f"ğŸ§  èƒŒæ™¯ï¼šåæ€å®Œæˆï¼ˆç½®ä¿¡åº¦: {reflection_result.get('confidence', 0):.2f}ï¼‰")
                
                # === éšæ®µ1.5ï¼šåæ€å„²å­˜ï¼ˆä¸‰å±¤æ¶æ§‹ï¼‰===
                reflection_storage = get_reflection_storage()
                if reflection_storage and reflection_result:
                    storage_result = await reflection_storage.store_reflection(
                        reflection_data=reflection_result,
                        conversation_id=request.conversation_id,
                        user_id=request.user_id,
                        related_message_id=None
                    )
                    if storage_result.get("overall_success"):
                        logger.info(f"ğŸ’¾ èƒŒæ™¯ï¼šåæ€å·²å„²å­˜åˆ°ä¸‰å±¤æ¶æ§‹")
                    else:
                        logger.warning(f"âš ï¸ èƒŒæ™¯ï¼šåæ€å„²å­˜éƒ¨åˆ†å¤±æ•—")
                
                # === éšæ®µ2ï¼šè¡Œç‚ºèª¿ç¯€ï¼ˆåŸºæ–¼åæ€çµæœï¼‰===
                behavior_module = await controller.get_module("behavior")
                if behavior_module and reflection_result:
                    behavior_response = await behavior_module.process({
                        "reflection": reflection_result,
                        "emotion_analysis": emotion_analysis,
                        "conversation_context": {
                            "user_message": request.user_message,
                            "assistant_message": assistant_message
                        }
                    })
                    if behavior_response.get("success"):
                        logger.info(f"ğŸ¯ èƒŒæ™¯ï¼šäººæ ¼èª¿æ•´å·²å®Œæˆ")
        
        # === éšæ®µ3ï¼šè¨˜æ†¶å„²å­˜ï¼ˆå«åæ€èˆ‡è¡Œç‚ºèª¿æ•´ï¼‰===
        new_memory = get_new_memory_core()
        if new_memory:
            result = new_memory.store_conversation(
                conversation_id=request.conversation_id,
                user_id=request.user_id,
                user_msg=request.user_message,
                assistant_msg=assistant_message,
                reflection=reflection_result
            )
            if result.get("success"):
                logger.info(f"ğŸ’¾ èƒŒæ™¯ï¼šæ–°è¨˜æ†¶æ¨¡çµ„å·²å„²å­˜ï¼ˆToken: {result.get('token_count', 0)}ï¼‰")
                
    except Exception as e:
        logger.warning(f"âš ï¸ èƒŒæ™¯ä»»å‹™è™•ç†å¤±æ•—: {e}", exc_info=True)

# =========================================================
# âœ… ä¸»æµç¨‹ï¼š/chat è·¯ç”±ï¼ˆåªè² è²¬å›è¦† - å·²æ•´åˆé›™ AI é‚è¼¯ï¼‰
# =========================================================
@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, background_tasks: BackgroundTasks):
    try:
        logger.info(f"ğŸŸ¢ æ¥æ”¶åˆ°èŠå¤©è«‹æ±‚ï¼Œconversation_id: {request.conversation_id}")
        openai_client = get_openai_client()
        memories_table = os.getenv("SUPABASE_MEMORIES_TABLE", "xiaochenguang_memories")

        memory_system = MemorySystem(supabase, openai_client, memories_table)
        prompt_engine = PromptEngine(request.conversation_id, memories_table)

        # 1. åŸ·è¡Œæ‰€æœ‰ã€Œè®€å–ã€ä»»å‹™ï¼ˆé€™å¿…é ˆæ˜¯åŒæ­¥çš„ï¼‰
        recalled_memories = await memory_system.recall_memories(
            request.user_message,
            request.conversation_id
        )
        conversation_history = memory_system.get_conversation_history(
            request.conversation_id,
            limit=5
        )

        # Retrieve file content from Redis (ä¿ç•™åŸé‚è¼¯)
        file_content = ""
        try:
            keys = redis_interface.redis.keys(f"upload:{request.conversation_id}:*")
            if keys:
                latest_key = keys[-1]
                file_data_json = redis_interface.redis.get(latest_key)
                if file_data_json:
                    file_data = json.loads(file_data_json)
                    file_content = file_data.get("content", "")
                    logger.info(f"ğŸ“„ æˆåŠŸå¾ Redis æª¢ç´¢æª”æ¡ˆå…§å®¹: {latest_key}")
        except Exception as e:
            logger.warning(f"âš ï¸ å¾ Redis æª¢ç´¢æª”æ¡ˆå…§å®¹å¤±æ•—: {e}")

        messages, emotion_analysis = await prompt_engine.build_prompt(
            request.user_message,
            recalled_memories,
            conversation_history,
            file_content
        )
        
        # âœ… ã€é›™ AI å¼•æ“å•Ÿå‹•ã€‘ï¼šæ ¹æ“š AI ID é¸æ“‡è¦è§€å¯Ÿçš„ AI å¯¶è²æ¨¡å‹
        if request.ai_id == "story_master_v1":
            # é€™æ˜¯ä½ ç¬¬ä¸€å€‹æ–°çš„æ•…äº‹å…‰å…‰å¯¶è²
            selected_model = "gpt-4o" # ä½¿ç”¨æ›´å¼·å¤§çš„æ¨¡å‹ä¾†ç·¨ç¹”è¤‡é›œæ•…äº‹
            selected_temperature = 0.95
            logger.info("âœ¨ å•Ÿç”¨ï¼šStory Master æ•…äº‹å…‰å…‰å¯¶è²")
        else:
            # é è¨­çš„å°æ™¨å…‰å¯¶è² (ç¶­æŒå¿«é€Ÿå’Œç¶“æ¿Ÿ)
            selected_model = "gpt-4o-mini"
            selected_temperature = 0.8
            logger.info("ğŸŒŸ å•Ÿç”¨ï¼šXiaochenguang é è¨­å…‰å…‰å¯¶è²")

        # 2. å‘¼å« OpenAI ç²å¾—å›è¦†ï¼ˆä¸»æµç¨‹çš„ç­‰å¾…é»ï¼‰
        assistant_message = await generate_response(
            openai_client,
            messages,
            model=selected_model, # <--- æ›¿æ›æˆé¸æ“‡çš„æ¨¡å‹
            max_tokens=1000,
            temperature=selected_temperature # <--- æ›¿æ›æˆé¸æ“‡çš„æº«åº¦
        )
        
        # 3. [é‡è¦] ä¿æŒæ ¸å¿ƒè¨˜æ†¶ç«‹å³å„²å­˜ (ç¢ºä¿ä¸»è¨Šæ¯ä¸æœƒä¸Ÿå¤±)
        await memory_system.save_memory(
            request.conversation_id, 
            request.user_message, 
            assistant_message,
            emotion_analysis, 
            ai_id=request.ai_id # âœ… ä½¿ç”¨å‚³å…¥çš„ AI ID
        )

        # âœ… ã€æ ¸å¿ƒå‹•ä½œã€‘å°‡æ‰€æœ‰è€—æ™‚çš„ã€Œæ¬å®¶éšŠä¼ã€æ¨å…¥å¾Œé–€é€šé“ï¼
        background_tasks.add_task(
            run_post_chat_tasks,
            request, 
            assistant_message, 
            emotion_analysis
        )

        # 4. ç«‹å³è¿”å›çµ¦ç”¨æˆ¶ï¼Œä¸å†ç­‰å¾…èƒŒæ™¯ä»»å‹™
        return ChatResponse(
            assistant_message=assistant_message,
            emotion_analysis=emotion_analysis,
            conversation_id=request.conversation_id,
            reflection=None # ä¸å†ç­‰å¾… reflection çµæœ
        )

    except Exception as e:
        traceback_str = traceback.format_exc()
        logger.error(f"ğŸ”¥ Chat Endpoint ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="è¼‰é«”å…§éƒ¨å…‰æµç•°å¸¸ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚")
