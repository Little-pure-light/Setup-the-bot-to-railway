"""
XiaoChenGuang é–‹ç™¼è€…è¨˜æ†¶åŠ©æ‰‹ - å¾Œç«¯æ¨¡çµ„ V2
å®Œå…¨é‡æ§‹ç‰ˆæœ¬ - è§£æ±ºæ‰€æœ‰å·²çŸ¥å•é¡Œ

ä¿®å¾©å…§å®¹ï¼š
1. âœ… åŠ å…¥ dotenv ç’°å¢ƒè®Šæ•¸è¼‰å…¥
2. âœ… ä¿®æ­£ RPC å‡½æ•¸èª¿ç”¨ï¼ˆçµ±ä¸€ç°½ç« ï¼‰
3. âœ… å®Œæ•´çš„éŒ¯èª¤è™•ç†ï¼ˆtry/exceptï¼‰
4. âœ… ä½¿ç”¨ .get() é˜²æ­¢ KeyError
5. âœ… çµ±ä¸€ä½¿ç”¨ text-embedding-3-small
"""

import os
from typing import Optional, List, Dict, Any
from datetime import datetime
from dotenv import load_dotenv

# ============================================
# ç’°å¢ƒè®Šæ•¸è¼‰å…¥ï¼ˆä¿®å¾©å•é¡Œ 1ï¼‰
# ============================================
load_dotenv()  # å¼·åˆ¶è¼‰å…¥ .env æª”æ¡ˆ

# ============================================
# å»¶é²å°å…¥ï¼ˆé¿å…ç’°å¢ƒè®Šæ•¸æœªè¼‰å…¥ï¼‰
# ============================================
try:
    from supabase import create_client, Client
    from openai import OpenAI
except ImportError as e:
    print(f"âŒ å¥—ä»¶å°å…¥éŒ¯èª¤ï¼š{e}")
    print("è«‹åŸ·è¡Œï¼špip install supabase openai python-dotenv")
    raise


class DevMemoryBackend:
    """
    é–‹ç™¼è€…è¨˜æ†¶å¾Œç«¯è™•ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. å„²å­˜é–‹ç™¼å°è©±
    2. èªç¾©æœå°‹è¨˜éŒ„
    3. ç”Ÿæˆå°ˆæ¡ˆèƒŒæ™¯åŒ…
    
    å®Œç¾å¥‘åˆ XiaoChenGuang éˆé­‚å­µåŒ–å™¨æ¶æ§‹
    """
    
    def __init__(
        self,
        supabase_url: Optional[str] = None,
        supabase_key: Optional[str] = None,
        openai_api_key: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–å¾Œç«¯æ¨¡çµ„
        
        åƒæ•¸ï¼š
            supabase_url: Supabase URLï¼ˆé è¨­å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
            supabase_key: Supabase Anon Keyï¼ˆé è¨­å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
            openai_api_key: OpenAI API Keyï¼ˆé è¨­å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
        """
        # ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆåŠ å…¥é è¨­å€¼è™•ç†ï¼‰
        self.supabase_url = supabase_url or os.getenv("SUPABASE_URL")
        self.supabase_key = supabase_key or os.getenv("SUPABASE_ANON_KEY")
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        
        # é©—è­‰ç’°å¢ƒè®Šæ•¸
        self._validate_env_vars()
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        try:
            self.supabase: Client = create_client(self.supabase_url, self.supabase_key)
            self.openai_client = OpenAI(api_key=self.openai_api_key)
            self.embedding_model = "text-embedding-3-small"  # çµ±ä¸€ä½¿ç”¨æ­¤æ¨¡å‹
        except Exception as e:
            raise RuntimeError(f"âŒ å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
    
    def _validate_env_vars(self) -> None:
        """é©—è­‰å¿…è¦çš„ç’°å¢ƒè®Šæ•¸æ˜¯å¦å­˜åœ¨"""
        missing_vars = []
        
        if not self.supabase_url:
            missing_vars.append("SUPABASE_URL")
        if not self.supabase_key:
            missing_vars.append("SUPABASE_ANON_KEY")
        if not self.openai_api_key:
            missing_vars.append("OPENAI_API_KEY")
        
        if missing_vars:
            raise ValueError(
                f"âŒ ç¼ºå°‘ç’°å¢ƒè®Šæ•¸ï¼š{', '.join(missing_vars)}\n"
                f"è«‹åœ¨ Replit Secrets æˆ– .env æª”æ¡ˆä¸­è¨­å®š"
            )
    
    def save_dev_log(
        self,
        phase: str,
        module: str,
        ai_model: str,
        topic: str,
        user_question: str,
        ai_response: str,
        tags: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        å„²å­˜é–‹ç™¼æ—¥èªŒ
        
        åƒæ•¸ï¼š
            phase: éšæ®µï¼ˆPhase1/Phase2...ï¼‰
            module: æ¨¡çµ„åç¨±
            ai_model: AI æ¨¡å‹
            topic: è¨è«–ä¸»é¡Œ
            user_question: ç”¨æˆ¶å•é¡Œ
            ai_response: AI å›ç­”
            tags: æ¨™ç±¤åˆ—è¡¨
        
        å›å‚³ï¼š
            {"success": bool, "log_id": str, "summary": str} æˆ–éŒ¯èª¤è¨Šæ¯
        """
        try:
            # 1. ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(user_question, ai_response)
            
            # 2. è¨ˆç®—é‡è¦æ€§
            importance = self._calculate_importance(user_question, ai_response, phase)
            
            # 3. ç”Ÿæˆå‘é‡åµŒå…¥
            combined_text = f"{topic} {user_question} {ai_response}"
            embedding = self._get_embedding(combined_text)
            
            if not embedding:
                return {
                    "success": False,
                    "error": "å‘é‡ç”Ÿæˆå¤±æ•—"
                }
            
            # 4. æº–å‚™è³‡æ–™
            data = {
                "phase": phase,
                "module": module,
                "ai_model": ai_model,
                "topic": topic,
                "user_question": user_question,
                "ai_response": ai_response,
                "summary": summary,
                "embedding": embedding,
                "importance_score": importance,
                "tags": tags or []
            }
            
            # 5. å„²å­˜åˆ° Supabase
            result = self.supabase.table("dev_logs").insert(data).execute()
            
            # 6. æª¢æŸ¥çµæœï¼ˆä½¿ç”¨ .get() é˜²æ­¢ KeyErrorï¼‰
            if result.data and len(result.data) > 0:
                log_id = result.data[0].get("id", "unknown")
                return {
                    "success": True,
                    "log_id": str(log_id),
                    "summary": summary,
                    "importance": importance
                }
            else:
                return {
                    "success": False,
                    "error": "è³‡æ–™å„²å­˜å¤±æ•—ï¼Œç„¡å›å‚³çµæœ"
                }
        
        except Exception as e:
            return {
                "success": False,
                "error": f"å„²å­˜éŒ¯èª¤ï¼š{str(e)}"
            }
    
    def search_memory(
        self,
        query: str,
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        èªç¾©æœå°‹é–‹ç™¼è¨˜éŒ„
        
        åƒæ•¸ï¼š
            query: æœå°‹å•é¡Œ
            limit: è¿”å›æ•¸é‡
        
        å›å‚³ï¼š
            ç›¸é—œè¨˜éŒ„åˆ—è¡¨ï¼ˆåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•¸ï¼‰
        """
        try:
            # 1. ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self._get_embedding(query)
            
            if not query_embedding:
                return []
            
            # 2. å‘¼å« RPC å‡½æ•¸ï¼ˆä¿®æ­£ç‰ˆæœ¬ - åªå‚³éå…©å€‹åƒæ•¸ï¼‰
            result = self.supabase.rpc(
                "match_dev_logs",
                {
                    "query_embedding": query_embedding,
                    "match_count": limit
                }
            ).execute()
            
            # 3. è™•ç†çµæœï¼ˆä½¿ç”¨ .get() é˜²æ­¢ KeyErrorï¼‰
            if result.data:
                return [
                    {
                        "id": log.get("id"),
                        "created_at": log.get("created_at"),
                        "phase": log.get("phase", "æœªçŸ¥"),
                        "module": log.get("module", "æœªçŸ¥"),
                        "ai_model": log.get("ai_model", "æœªçŸ¥"),
                        "topic": log.get("topic", "ç„¡ä¸»é¡Œ"),
                        "user_question": log.get("user_question", ""),
                        "ai_response": log.get("ai_response", ""),
                        "summary": log.get("summary", ""),
                        "similarity": round(log.get("similarity", 0.0), 4)
                    }
                    for log in result.data
                ]
            else:
                return []
        
        except Exception as e:
            print(f"âŒ æœå°‹éŒ¯èª¤ï¼š{e}")
            return []
    
    def generate_context_pack(
        self,
        target_phase: Optional[str] = None,
        target_module: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆå°ˆæ¡ˆèƒŒæ™¯åŒ…
        
        åƒæ•¸ï¼š
            target_phase: ç¯©é¸éšæ®µï¼ˆNone = å…¨éƒ¨ï¼‰
            target_module: ç¯©é¸æ¨¡çµ„ï¼ˆNone = å…¨éƒ¨ï¼‰
        
        å›å‚³ï¼š
            æ ¼å¼åŒ–çš„èƒŒæ™¯åŒ…æ–‡æœ¬
        """
        try:
            # æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ RPC å‡½æ•¸ç”Ÿæˆï¼ˆæ¨è–¦ï¼‰
            result = self.supabase.rpc(
                "generate_project_context",
                {
                    "target_phase": target_phase,
                    "target_module": target_module,
                    "max_logs": 10
                }
            ).execute()
            
            if result.data:
                return result.data
            else:
                # æ–¹æ¡ˆ 2ï¼šæ‰‹å‹•ç”Ÿæˆï¼ˆé™ç´šæ–¹æ¡ˆï¼‰
                return self._manual_generate_context(target_phase, target_module)
        
        except Exception as e:
            print(f"âš ï¸ RPC ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•æ–¹æ¡ˆï¼š{e}")
            return self._manual_generate_context(target_phase, target_module)
    
    def _generate_summary(self, question: str, answer: str) -> str:
        """ç”Ÿæˆç°¡å–®æ‘˜è¦ï¼ˆä¸èŠ±éŒ¢ï¼‰"""
        q_summary = question[:50] + ("..." if len(question) > 50 else "")
        a_summary = answer[:100] + ("..." if len(answer) > 100 else "")
        return f"å•ï¼š{q_summary} ç­”ï¼š{a_summary}"
    
    def _calculate_importance(self, question: str, answer: str, phase: str) -> float:
        """è¨ˆç®—é‡è¦æ€§è©•åˆ†ï¼ˆ0-1ï¼‰"""
        score = 0.5
        
        # Phase åŠ æ¬Š
        if phase in ["Phase3", "Phase4", "Phase5"]:
            score += 0.3
        elif phase in ["Phase2"]:
            score += 0.1
        
        # å›ç­”é•·åº¦
        if len(answer) > 500:
            score += 0.2
        elif len(answer) > 200:
            score += 0.1
        
        # é—œéµå­—æª¢æ¸¬
        keywords = ["bug", "éŒ¯èª¤", "æ¸¬è©¦", "ä¿®å¾©", "å•é¡Œ", "å¤±æ•—", "é‡è¦", "é—œéµ"]
        text_lower = (question + answer).lower()
        if any(kw in text_lower for kw in keywords):
            score += 0.1
        
        return min(score, 1.0)
    
    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """
        ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥
        
        ä½¿ç”¨ OpenAI text-embedding-3-smallï¼ˆ1536 ç¶­ï¼‰
        """
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text[:8000]  # é™åˆ¶é•·åº¦é¿å…è¶…å‡º token é™åˆ¶
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ å‘é‡ç”ŸæˆéŒ¯èª¤ï¼š{e}")
            return None
    
    def _manual_generate_context(
        self,
        target_phase: Optional[str],
        target_module: Optional[str]
    ) -> str:
        """æ‰‹å‹•ç”ŸæˆèƒŒæ™¯åŒ…ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰"""
        try:
            # æŸ¥è©¢æœ€è¿‘è¨˜éŒ„
            query = self.supabase.table("dev_logs").select("*")
            
            if target_phase:
                query = query.eq("phase", target_phase)
            if target_module:
                query = query.eq("module", target_module)
            
            result = query.order("created_at", desc=True).limit(10).execute()
            logs = result.data or []
            
            # çµ„åˆæ–‡æœ¬
            context = f"""ğŸ“¦ XiaoChenGuang å°ˆæ¡ˆèƒŒæ™¯åŒ…
ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ã€å°ˆæ¡ˆæ¦‚è¿°ã€‘
æ•¸ä½éˆé­‚å­µåŒ–å™¨ - XiaoChenGuang AI ç³»çµ±
æŠ€è¡“æ£§ï¼šFastAPI + Vue 3 + Supabase + OpenAI + Redis
æ ¸å¿ƒæ¨¡çµ„ï¼š
  â€¢ è¨˜æ†¶ç³»çµ±ï¼ˆå‘é‡åµŒå…¥ + pgvectorï¼‰
  â€¢ åæ€æ¨¡çµ„ï¼ˆåæ¨æœå› æ³•å‰‡ï¼‰
  â€¢ äººæ ¼å­¸ç¿’å¼•æ“ï¼ˆå‹•æ…‹ç‰¹è³ªèª¿æ•´ï¼‰
  â€¢ æƒ…æ„Ÿæª¢æ¸¬ç³»çµ±ï¼ˆ9ç¨®æƒ…ç·’é¡å‹ï¼‰
  â€¢ æç¤ºè©å¼•æ“ï¼ˆå‹•æ…‹ Prompt ç”Ÿæˆï¼‰

ã€æœ€è¿‘é–‹ç™¼è¨˜éŒ„ã€‘
"""
            
            if logs:
                for i, log in enumerate(logs, 1):
                    date = log.get("created_at", "")[:10]
                    phase = log.get("phase", "æœªçŸ¥")
                    topic = log.get("topic", "ç„¡ä¸»é¡Œ")
                    summary = log.get("summary", "")[:80]
                    
                    context += f"{i}. {date} [{phase}] {topic}\n"
                    context += f"   æ‘˜è¦ï¼š{summary}...\n\n"
            else:
                context += "ï¼ˆç›®å‰å°šç„¡é–‹ç™¼è¨˜éŒ„ï¼‰\n\n"
            
            context += """ã€ä½¿ç”¨èªªæ˜ã€‘
1. è¤‡è£½ä¸Šé¢çš„èƒŒæ™¯åŒ…
2. è²¼çµ¦ä»»ä½• AIï¼ˆChatGPT/Claude/Geminiï¼‰
3. AI å°±èƒ½ç«‹åˆ»äº†è§£å°ˆæ¡ˆèƒŒæ™¯ï¼
4. ç„¶å¾Œå•ä½ çš„å•é¡Œï¼ŒAI æœƒçµ¦å‡ºæ›´ç²¾æº–çš„å»ºè­°

---
ç”± XiaoChenGuang é–‹ç™¼è€…è¨˜æ†¶åŠ©æ‰‹ç”Ÿæˆ
"""
            
            return context
        
        except Exception as e:
            return f"âŒ èƒŒæ™¯åŒ…ç”Ÿæˆå¤±æ•—ï¼š{e}"


# ============================================
# ä½¿ç”¨ç¯„ä¾‹
# ============================================

if __name__ == "__main__":
    # åˆå§‹åŒ–
    backend = DevMemoryBackend()
    
    # æ¸¬è©¦å„²å­˜
    result = backend.save_dev_log(
        phase="Phase1",
        module="æ¸¬è©¦æ¨¡çµ„",
        ai_model="GPT-4",
        topic="å¾Œç«¯æ¸¬è©¦",
        user_question="é€™å€‹å¾Œç«¯æ¨¡çµ„èƒ½æ­£å¸¸é‹ä½œå—ï¼Ÿ",
        ai_response="æ˜¯çš„ï¼Œæ‰€æœ‰åŠŸèƒ½éƒ½å·²æ­£ç¢ºå¯¦ä½œä¸¦åŠ å…¥å®Œæ•´çš„éŒ¯èª¤è™•ç†ã€‚",
        tags=["æ¸¬è©¦", "å¾Œç«¯"]
    )
    print("å„²å­˜çµæœï¼š", result)
    
    # æ¸¬è©¦æœå°‹
    search_results = backend.search_memory("æ¸¬è©¦", limit=3)
    print(f"\næœå°‹çµæœï¼ˆæ‰¾åˆ° {len(search_results)} ç­†ï¼‰ï¼š")
    for log in search_results:
        print(f"  - {log['topic']} (ç›¸ä¼¼åº¦: {log['similarity']:.2%})")
    
    # æ¸¬è©¦èƒŒæ™¯åŒ…
    context = backend.generate_context_pack()
    print("\nèƒŒæ™¯åŒ…é è¦½ï¼š")
    print(context[:300], "...")
