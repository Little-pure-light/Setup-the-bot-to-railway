"""
é–‹ç™¼è€…è¨˜æ†¶æ¨¡çµ„ - å¾Œç«¯è™•ç†å™¨
ç”¨é€”ï¼šæ•´åˆåˆ°ä½ ç¾æœ‰çš„ XiaoChenGuang éˆé­‚å­µåŒ–å™¨ç³»çµ±
ä½ç½®ï¼šæ”¾åœ¨ backend/modules/dev_memory.py

å®Œç¾å¥‘åˆä½ ç¾æœ‰çš„æ¶æ§‹ï¼š
- ä½¿ç”¨ç›¸åŒçš„ Supabase é€£æ¥
- ä½¿ç”¨ç›¸åŒçš„ OpenAI embeddings
- API é¢¨æ ¼è·Ÿç¾æœ‰çš„ memory_router.py ä¸€è‡´
"""

from typing import Optional, List, Dict
from datetime import datetime
import openai
from supabase import Client


class DevMemoryModule:
    """
    é–‹ç™¼è€…è¨˜æ†¶æ¨¡çµ„
    
    åŠŸèƒ½ï¼š
    1. å„²å­˜é–‹ç™¼å°è©±ï¼ˆä½ è·Ÿå„ç¨® AI çš„è¨è«–ï¼‰
    2. èªç¾©æœå°‹ï¼ˆæ ¹æ“šå•é¡Œæ‰¾ç›¸é—œè¨˜éŒ„ï¼‰
    3. ç”Ÿæˆå°ˆæ¡ˆèƒŒæ™¯åŒ…ï¼ˆå–šé†’æ–° AI çš„è¨˜æ†¶ï¼‰
    """
    
    def __init__(self, supabase_client: Client, openai_api_key: str):
        """
        åˆå§‹åŒ–æ¨¡çµ„
        
        åƒæ•¸ï¼š
            supabase_client: Supabase å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨ç¾æœ‰çš„é€£æ¥ï¼‰
            openai_api_key: OpenAI API é‡‘é‘°ï¼ˆä½¿ç”¨ç¾æœ‰çš„ï¼‰
        """
        self.supabase = supabase_client
        openai.api_key = openai_api_key
        self.embedding_model = "text-embedding-3-small"  # è·Ÿä½ çš„ memory_system.py ä¸€æ¨£
    
    async def save_dev_log(
        self,
        phase: str,
        module: str,
        ai_model: str,
        topic: str,
        user_question: str,
        ai_response: str,
        tags: Optional[List[str]] = None,
        related_files: Optional[List[str]] = None
    ) -> Dict:
        """
        å„²å­˜é–‹ç™¼æ—¥èªŒ
        
        åƒæ•¸ï¼š
            phase: éšæ®µï¼ˆPhase1/Phase2/Phase3...ï¼‰
            module: æ¨¡çµ„åç¨±ï¼ˆè¨˜æ†¶æ¨¡çµ„/åæ€æ¨¡çµ„...ï¼‰
            ai_model: AI æ¨¡å‹ï¼ˆGPT-4/Claude/Geminiï¼‰
            topic: è¨è«–ä¸»é¡Œ
            user_question: ä½ çš„å•é¡Œ
            ai_response: AI çš„å›ç­”
            tags: æ¨™ç±¤åˆ—è¡¨
            related_files: ç›¸é—œæª”æ¡ˆè·¯å¾‘
        
        å›å‚³ï¼š
            å„²å­˜çµæœ
        """
        try:
            # 1. ç”Ÿæˆæ‘˜è¦ï¼ˆç°¡å–®è¦å‰‡ï¼Œä¸èŠ±éŒ¢ï¼‰
            summary = self._generate_summary(user_question, ai_response)
            
            # 2. è¨ˆç®—é‡è¦æ€§è©•åˆ†
            importance = self._calculate_importance(user_question, ai_response, phase)
            
            # 3. ç”Ÿæˆå‘é‡åµŒå…¥
            combined_text = f"{topic} {user_question} {ai_response}"
            embedding = await self._get_embedding(combined_text)
            
            # 4. å„²å­˜åˆ° Supabase
            data = {
                "phase": phase,
                "module": module,
                "ai_model": ai_model,
                "topic": topic,
                "user_question": user_question,
                "ai_response": ai_response,
                "summary": summary,
                "embedding": embedding,
                "importance_score": importance,
                "tags": tags or [],
                "related_files": related_files or []
            }
            
            result = self.supabase.table("dev_logs").insert(data).execute()
            
            return {
                "success": True,
                "log_id": result.data[0]["id"],
                "summary": summary,
                "importance": importance
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def search_dev_logs(
        self,
        query: str,
        limit: int = 5,
        filter_phase: Optional[str] = None,
        filter_module: Optional[str] = None
    ) -> List[Dict]:
        """
        èªç¾©æœå°‹é–‹ç™¼æ—¥èªŒ
        
        åƒæ•¸ï¼š
            query: æœå°‹å•é¡Œï¼ˆä¾‹å¦‚ï¼šã€Œåæ€æ¨¡çµ„å¦‚ä½•æ¸¬è©¦ï¼Ÿã€ï¼‰
            limit: è¿”å›æ•¸é‡
            filter_phase: ç¯©é¸éšæ®µ
            filter_module: ç¯©é¸æ¨¡çµ„
        
        å›å‚³ï¼š
            ç›¸é—œæ—¥èªŒåˆ—è¡¨
        """
        try:
            # 1. å°‡å•é¡Œè½‰æ›ç‚ºå‘é‡
            query_embedding = await self._get_embedding(query)
            
            # 2. å‘¼å« Supabase RPC å‡½æ•¸
            result = self.supabase.rpc(
                "match_dev_logs",
                {
                    "query_embedding": query_embedding,
                    "match_count": limit,
                    "filter_phase": filter_phase,
                    "filter_module": filter_module
                }
            ).execute()
            
            return result.data
        
        except Exception as e:
            print(f"æœå°‹éŒ¯èª¤: {e}")
            return []
    
    async def generate_context_pack(
        self,
        target_phase: Optional[str] = None,
        target_module: Optional[str] = None,
        include_recent: int = 10
    ) -> str:
        """
        ç”Ÿæˆå°ˆæ¡ˆèƒŒæ™¯åŒ…ï¼ˆçµ¦æ–° AI çœ‹çš„ï¼‰
        
        åƒæ•¸ï¼š
            target_phase: ç›®æ¨™éšæ®µï¼ˆNone = å…¨éƒ¨ï¼‰
            target_module: ç›®æ¨™æ¨¡çµ„ï¼ˆNone = å…¨éƒ¨ï¼‰
            include_recent: åŒ…å«æœ€è¿‘ N ç­†è¨˜éŒ„
        
        å›å‚³ï¼š
            æ ¼å¼åŒ–çš„èƒŒæ™¯åŒ…æ–‡æœ¬
        """
        try:
            # å‘¼å« Supabase RPC å‡½æ•¸
            result = self.supabase.rpc(
                "generate_project_context",
                {
                    "target_phase": target_phase,
                    "target_module": target_module
                }
            ).execute()
            
            return result.data
        
        except Exception as e:
            # é™ç´šæ–¹æ¡ˆï¼šæ‰‹å‹•ç”Ÿæˆ
            return self._manual_generate_context(target_phase, target_module, include_recent)
    
    def _generate_summary(self, question: str, answer: str) -> str:
        """
        ç”Ÿæˆæ‘˜è¦ï¼ˆç°¡å–®è¦å‰‡ï¼Œä¸èŠ±éŒ¢ï¼‰
        
        è¦å‰‡ï¼š
        1. æå–å•é¡Œçš„å‰ 50 å­—
        2. æå–ç­”æ¡ˆçš„å‰ 100 å­—
        3. çµ„åˆæˆæ‘˜è¦
        """
        q_summary = question[:50] + ("..." if len(question) > 50 else "")
        a_summary = answer[:100] + ("..." if len(answer) > 100 else "")
        
        return f"å•ï¼š{q_summary} ç­”ï¼š{a_summary}"
    
    def _calculate_importance(self, question: str, answer: str, phase: str) -> float:
        """
        è¨ˆç®—é‡è¦æ€§è©•åˆ†ï¼ˆ0-1ï¼‰
        
        è¦å‰‡ï¼š
        1. Phase 3/4 = é«˜é‡è¦æ€§ï¼ˆ0.8-1.0ï¼‰
        2. é•·å›ç­” = è¼ƒé‡è¦ï¼ˆ>500å­— +0.2ï¼‰
        3. åŒ…å«é—œéµå­— = è¼ƒé‡è¦ï¼ˆbug/éŒ¯èª¤/æ¸¬è©¦ +0.1ï¼‰
        """
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # Phase åŠ æ¬Š
        if phase in ["Phase3", "Phase4"]:
            score += 0.3
        
        # å›ç­”é•·åº¦
        if len(answer) > 500:
            score += 0.2
        
        # é—œéµå­—æª¢æ¸¬
        keywords = ["bug", "éŒ¯èª¤", "æ¸¬è©¦", "ä¿®å¾©", "å•é¡Œ", "å¤±æ•—"]
        if any(kw in question.lower() or kw in answer.lower() for kw in keywords):
            score += 0.1
        
        return min(score, 1.0)  # ä¸Šé™ 1.0
    
    async def _get_embedding(self, text: str) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥
        
        ä½¿ç”¨ OpenAI text-embedding-3-smallï¼ˆè·Ÿä½ çš„ memory_system.py ä¸€æ¨£ï¼‰
        """
        response = openai.embeddings.create(
            model=self.embedding_model,
            input=text
        )
        return response.data[0].embedding
    
    def _manual_generate_context(
        self,
        target_phase: Optional[str],
        target_module: Optional[str],
        limit: int
    ) -> str:
        """
        æ‰‹å‹•ç”ŸæˆèƒŒæ™¯åŒ…ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰
        """
        # æŸ¥è©¢æœ€è¿‘è¨˜éŒ„
        query = self.supabase.table("dev_logs").select("*")
        
        if target_phase:
            query = query.eq("phase", target_phase)
        if target_module:
            query = query.eq("module", target_module)
        
        result = query.order("created_at", desc=True).limit(limit).execute()
        logs = result.data
        
        # çµ„åˆæ–‡æœ¬
        context = f"ğŸ“¦ XiaoChenGuang å°ˆæ¡ˆèƒŒæ™¯åŒ…\n"
        context += f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
        
        context += "ã€å°ˆæ¡ˆæ¦‚è¿°ã€‘\n"
        context += "æ•¸ä½éˆé­‚å­µåŒ–å™¨ - XiaoChenGuang AI ç³»çµ±\n"
        context += "- è¨˜æ†¶æ¨¡çµ„ï¼ˆå‘é‡åµŒå…¥ + pgvectorï¼‰\n"
        context += "- åæ€æ¨¡çµ„ï¼ˆåæ¨æœå› æ³•å‰‡ï¼‰\n"
        context += "- è¡Œç‚ºèª¿ç¯€æ¨¡çµ„ï¼ˆäººæ ¼å‘é‡ï¼‰\n"
        context += "- æƒ…æ„Ÿæª¢æ¸¬ç³»çµ±ï¼ˆ9ç¨®æƒ…ç·’ï¼‰\n\n"
        
        context += "ã€æœ€è¿‘é–‹ç™¼è¨˜éŒ„ã€‘\n"
        for log in logs:
            date = log["created_at"][:10]
            phase = log.get("phase", "é€šç”¨")
            topic = log.get("topic", "ç„¡ä¸»é¡Œ")
            summary = log.get("summary", "")[:100]
            
            context += f"- {date} [{phase}] {topic}\n"
            context += f"  æ‘˜è¦ï¼š{summary}...\n"
        
        context += "\nã€ä½¿ç”¨èªªæ˜ã€‘\n"
        context += "è«‹å°‡æ­¤èƒŒæ™¯åŒ…è¤‡è£½è²¼çµ¦ä»»ä½• AIï¼ŒAI å°±èƒ½äº†è§£å°ˆæ¡ˆèƒŒæ™¯ï¼\n"
        
        return context


# ============================================
# FastAPI è·¯ç”±ç¯„ä¾‹ï¼ˆæ•´åˆåˆ°ä½ çš„ backend/main.pyï¼‰
# ============================================

"""
from fastapi import APIRouter, Depends
from backend.modules.dev_memory import DevMemoryModule
from backend.supabase_handler import get_supabase_client

dev_router = APIRouter(prefix="/api/dev-memory", tags=["é–‹ç™¼è€…è¨˜æ†¶"])

@dev_router.post("/save")
async def save_dev_log(
    phase: str,
    module: str,
    ai_model: str,
    topic: str,
    user_question: str,
    ai_response: str,
    tags: List[str] = None
):
    dev_memory = DevMemoryModule(
        supabase_client=get_supabase_client(),
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    
    result = await dev_memory.save_dev_log(
        phase=phase,
        module=module,
        ai_model=ai_model,
        topic=topic,
        user_question=user_question,
        ai_response=ai_response,
        tags=tags
    )
    
    return result

@dev_router.get("/search")
async def search_logs(
    query: str,
    limit: int = 5,
    phase: str = None,
    module: str = None
):
    dev_memory = DevMemoryModule(...)
    results = await dev_memory.search_dev_logs(
        query=query,
        limit=limit,
        filter_phase=phase,
        filter_module=module
    )
    return results

@dev_router.get("/context-pack")
async def get_context_pack(
    phase: str = None,
    module: str = None
):
    dev_memory = DevMemoryModule(...)
    context = await dev_memory.generate_context_pack(
        target_phase=phase,
        target_module=module
    )
    return {"context": context}
"""
