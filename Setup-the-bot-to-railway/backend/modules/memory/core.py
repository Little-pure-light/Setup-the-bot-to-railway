"""
è¨˜æ†¶æ ¸å¿ƒ - Memory Core
AI æ•¸å­—å®‡å®™çš„ä¸­æ¨ç¥ç¶“è³‡æ–™å±¤æ ¸å¿ƒæ§åˆ¶å™¨

è² è²¬å”èª¿ï¼š
1. Token åŒ–è™•ç†
2. Redis çŸ­æœŸè¨˜æ†¶
3. Supabase é•·æœŸè¨˜æ†¶
4. IPFS ç´¢å¼•ï¼ˆé ç•™ï¼‰
5. æ¨¡çµ„é–“é€šä¿¡
"""
import json
from typing import Dict, Any, Optional, List
from datetime import datetime

from .tokenizer import TokenizerEngine
from .redis_interface import RedisInterface
from .supabase_interface import SupabaseInterface
from .io_contract import validate_and_normalize, create_memory_record
from backend.modules.ipfs_handler import get_ipfs_handler

class MemoryCore:
    """è¨˜æ†¶æ ¸å¿ƒæ§åˆ¶å™¨"""
    
    def __init__(
        self, 
        config: Optional[Dict[str, Any]] = None,
        redis_client=None,
        supabase_client=None
    ):
        """
        åˆå§‹åŒ–è¨˜æ†¶æ ¸å¿ƒ
        
        åƒæ•¸:
            config: æ¨¡çµ„é…ç½®
            redis_client: Redis å®¢æˆ¶ç«¯
            supabase_client: Supabase å®¢æˆ¶ç«¯
        """
        self.config = config or {}
        
        print("ğŸ§  æ­£åœ¨åˆå§‹åŒ–è¨˜æ†¶æ ¸å¿ƒ...")
        
        self.tokenizer = TokenizerEngine(
            tokenizer_name=self.config.get("tokenizer_name")
        )
        
        self.redis = RedisInterface(redis_client=redis_client)
        
        self.supabase = SupabaseInterface(
            supabase_client=supabase_client,
            config=self.config
        )
        
        # IPFS è™•ç†å™¨ï¼ˆç”¨æ–¼ç”Ÿæˆ CIDï¼‰
        self.ipfs = get_ipfs_handler()
        
        print("âœ… è¨˜æ†¶æ ¸å¿ƒåˆå§‹åŒ–å®Œæˆï¼ˆå« IPFS CID ç”Ÿæˆï¼‰")
    
    def save_chat(
        self, 
        user_message: str, 
        assistant_message: str, 
        conversation_id: str, 
        user_id: str,
        reflection: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å„²å­˜å°è©±å…§å®¹ï¼ˆçŸ­æœŸ + é•·æœŸï¼‰
        
        åƒæ•¸:
            user_message: ä½¿ç”¨è€…è¨Šæ¯
            assistant_message: AI å›è¦†
            conversation_id: å°è©± ID
            user_id: ä½¿ç”¨è€… ID
            reflection: åæ€çµæœï¼ˆå¯é¸ï¼‰
        
        è¿”å›:
            å„²å­˜çµæœ
        """
        try:
            token_data = self.tokenizer.pack_token_record(
                user=user_message,
                assistant=assistant_message,
                reflection_json=reflection
            )
            
            # ç”Ÿæˆå°è©±çš„ CIDï¼ˆå…§å®¹è­˜åˆ¥ç¬¦ï¼‰
            cid = self.ipfs.generate_conversation_cid(
                user_message=user_message,
                assistant_message=assistant_message,
                timestamp=datetime.utcnow().isoformat()
            )
            
            memory_record = create_memory_record(
                conversation_id=conversation_id,
                user_id=user_id,
                user_message=user_message,
                assistant_message=assistant_message,
                reflection=reflection,
                token_data=token_data,
                cid=cid
            )
            
            redis_data = {
                "user_msg": user_message,
                "assistant_msg": assistant_message,
                "reflection": reflection,
                "token_data": token_data,
                "user_id": user_id,
                "timestamp": int(datetime.utcnow().timestamp())
            }
            
            redis_success = self.redis.store_short_term(
                conversation_id, 
                redis_data
            )
            
            # âš¡ å³æ™‚å°è©±åªå­˜ Redisï¼ˆæå‡é€Ÿåº¦ï¼Œé™ä½è³‡æ–™åº«è² æ“”ï¼‰
            # ğŸ’¾ é•·æœŸå„²å­˜ä½¿ç”¨ flush_redis_to_supabase() æ‰¹æ¬¡åˆ·å¯«
            # supabase_success = self.supabase.store_single_memory(memory_record)
            
            return {
                "success": True,
                "redis_stored": redis_success,
                "supabase_stored": False,  # æ”¹ç‚ºæ‰¹æ¬¡å¯«å…¥æ¨¡å¼
                "token_count": token_data.get("total_count", 0),
                "conversation_id": conversation_id,
                "cid": cid,
                "note": "å³æ™‚å°è©±å·²å­˜å…¥ Redisï¼Œä½¿ç”¨ flush_redis_to_supabase() æ‰¹æ¬¡å¯«å…¥é•·æœŸå„²å­˜"
            }
            
        except Exception as e:
            print(f"âŒ å„²å­˜å°è©±å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def save_reflection(
        self, 
        reflection_text: str, 
        conversation_id: str,
        user_id: str,
        feedback_loop_id: Optional[str] = None,
        upload_to_ipfs: bool = False
    ) -> Dict[str, Any]:
        """
        å„²å­˜åæ€ç´€éŒ„
        
        åƒæ•¸:
            reflection_text: åæ€å…§å®¹
            conversation_id: å°è©± ID
            user_id: ä½¿ç”¨è€… ID
            feedback_loop_id: åé¥‹å¾ªç’° ID
            upload_to_ipfs: æ˜¯å¦ä¸Šå‚³åˆ° IPFS
        
        è¿”å›:
            å„²å­˜çµæœ
        """
        try:
            if isinstance(reflection_text, dict):
                reflection_data = reflection_text
                reflection_text_str = json.dumps(reflection_text, ensure_ascii=False)
            else:
                reflection_data = {"summary": reflection_text}
                reflection_text_str = reflection_text
            
            token_data = self.tokenizer.pack_token_record(
                reflection_json=reflection_data
            )
            
            cid = None
            if upload_to_ipfs:
                cid = self.upload_to_ipfs(reflection_text_str)
            
            reflection_record = {
                "conversation_id": conversation_id,
                "user_id": user_id,
                "reflection": reflection_data,
                "token_data": token_data,
                "cid": cid,
                "feedback_loop_id": feedback_loop_id,
                "created_at": datetime.utcnow().isoformat()
            }
            
            reflection_id = self.supabase.store_reflection(reflection_record)
            
            return {
                "success": True,
                "reflection_id": reflection_id,
                "cid": cid,
                "token_count": token_data.get("total_count", 0)
            }
            
        except Exception as e:
            print(f"âŒ å„²å­˜åæ€å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def store_conversation(
        self,
        conversation_id: str,
        user_id: str,
        user_msg: Optional[str] = None,
        assistant_msg: Optional[str] = None,
        reflection: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        çµ±ä¸€å„²å­˜ä»‹é¢ï¼ˆç”¨æ–¼è·¯ç”±èª¿ç”¨ï¼‰
        
        åƒæ•¸:
            conversation_id: å°è©± ID
            user_id: ä½¿ç”¨è€… ID
            user_msg: ä½¿ç”¨è€…è¨Šæ¯
            assistant_msg: AI å›è¦†
            reflection: åæ€çµæœ
        
        è¿”å›:
            å„²å­˜çµæœ
        """
        if user_msg and assistant_msg:
            return self.save_chat(
                user_message=user_msg,
                assistant_message=assistant_msg,
                conversation_id=conversation_id,
                user_id=user_id,
                reflection=reflection
            )
        else:
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…è¦çš„è¨Šæ¯å…§å®¹"
            }
    
    def load_recent_context(
        self, 
        conversation_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        è®€å–æœ€è¿‘çš„å°è©±ä¸Šä¸‹æ–‡
        
        åƒæ•¸:
            conversation_id: å°è©± ID
        
        è¿”å›:
            ä¸Šä¸‹æ–‡è³‡æ–™æˆ– None
        """
        redis_context = self.redis.load_recent_context(conversation_id)
        
        if redis_context:
            return redis_context
        
        supabase_memories = self.supabase.get_conversation_memories(
            conversation_id, 
            limit=1
        )
        
        if supabase_memories and len(supabase_memories) > 0:
            latest = supabase_memories[0]
            return {
                "user_msg": latest.get("user_message"),
                "assistant_msg": latest.get("assistant_message"),
                "reflection": latest.get("reflection"),
                "token_data": latest.get("token_data"),
                "user_id": latest.get("user_id"),
                "ts": latest.get("created_at")
            }
        
        return None
    
    def get_memory_context(
        self, 
        conversation_id: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ç²å–å®Œæ•´çš„å°è©±è¨˜æ†¶ä¸Šä¸‹æ–‡
        
        åƒæ•¸:
            conversation_id: å°è©± ID
            limit: æœ€å¤§è¿”å›æ•¸é‡
        
        è¿”å›:
            è¨˜æ†¶ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        redis_history = self.redis.get_conversation_history(conversation_id, limit)
        
        if redis_history:
            return redis_history
        
        return self.supabase.get_conversation_memories(conversation_id, limit)
    
    def get_reflection_cid(
        self, 
        conversation_id: str
    ) -> Optional[str]:
        """
        ç²å–åæ€çš„ IPFS CID
        
        åƒæ•¸:
            conversation_id: å°è©± ID
        
        è¿”å›:
            CID æˆ– None
        """
        reflections = self.supabase.get_user_reflections(conversation_id, limit=1)
        
        if reflections and len(reflections) > 0:
            return reflections[0].get("cid")
        
        return None
    
    def upload_to_ipfs(self, content: str) -> Optional[str]:
        """
        ä¸Šå‚³å…§å®¹åˆ° IPFSï¼ˆé ç•™åŠŸèƒ½ï¼‰
        
        åƒæ•¸:
            content: å…§å®¹
        
        è¿”å›:
            CID æˆ– None
        """
        print("âš ï¸ IPFS åŠŸèƒ½å°šæœªå¯¦ç¾ï¼Œå°‡åœ¨æœªä¾†ç‰ˆæœ¬ä¸­å•Ÿç”¨")
        return None
    
    def persist_long_term(
        self, 
        records: List[Dict[str, Any]]
    ) -> int:
        """
        æ‰¹æ¬¡æŒä¹…åŒ–åˆ°é•·æœŸè¨˜æ†¶
        
        åƒæ•¸:
            records: è¨˜éŒ„åˆ—è¡¨
        
        è¿”å›:
            æˆåŠŸå¯«å…¥çš„ç­†æ•¸
        """
        return self.supabase.commit_to_longterm(records)
    
    def flush_redis_to_supabase(
        self, 
        batch_size: int = 100
    ) -> Dict[str, Any]:
        """
        å°‡ Redis å¾…åˆ·å¯«éšŠåˆ—æ‰¹æ¬¡å¯«å…¥ Supabase
        
        åƒæ•¸:
            batch_size: æ‰¹æ¬¡å¤§å°
        
        è¿”å›:
            åˆ·å¯«çµæœ
        """
        try:
            pending_records = self.redis.get_pending_records(batch_size)
            
            if not pending_records:
                return {
                    "success": True,
                    "flushed_count": 0,
                    "message": "æ²’æœ‰å¾…åˆ·å¯«è¨˜éŒ„"
                }
            
            records_to_persist = []
            for item in pending_records:
                conversation_id = item.get("conversation_id")
                data = item.get("data", {})
                
                record = create_memory_record(
                    conversation_id=conversation_id,
                    user_id=data.get("user_id"),
                    user_message=data.get("user_msg"),
                    assistant_message=data.get("assistant_msg"),
                    reflection=data.get("reflection"),
                    token_data=data.get("token_data")
                )
                
                records_to_persist.append(record)
            
            success_count = self.persist_long_term(records_to_persist)
            
            return {
                "success": True,
                "total_pending": len(pending_records),
                "flushed_count": success_count,
                "failed_count": len(pending_records) - success_count
            }
            
        except Exception as e:
            print(f"âŒ åˆ·å¯«å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æª¢æŸ¥
        
        è¿”å›:
            å¥åº·ç‹€æ…‹
        """
        return {
            "status": "healthy",
            "tokenizer": {
                "method": "tiktoken" if not self.tokenizer.fallback_mode else "utf8_bytes",
                "encoding": self.tokenizer.tokenizer_name
            },
            "redis": self.redis.get_stats(),
            "supabase": self.supabase.get_stats()
        }
